# Sa se scrie un tool care primeste un hash pentru un fisier originial si o arhiva trunchiata
# (maxim “x” bytes de la finalul arhivei originale lipsesc).
# Tool-ul va gasi fisierul original din arhiva
# ( in momentul in care se va despacheta fisierul din arhiva va avea acelasi hash ca cel primit la input )
# Pentru rezolvarea acestei probleme este imperativa folosirea oricarei forme de paralelizare (
#
# multi threding/multiuprocessing / multi system / etc )
# INPUT:
# Arhiva truncata
# Numele fisierului din arhiva
# Hash-ul expected al fisierului
# Optional ar fi utila si o optiune de trunchiere a unei arhive si generare a datelor de input de
# mai sus.
# OUTPUT:
# Continutul fisierului dupa ce a fost dezarhivat cu success
import zipfile
from multiprocessing import Process, Queue, Lock, Pipe
from consumer_producer_model import producer, consumer
from file_processing import trim_archive, compute_hash_unopened_file, append_bytes_to_file


def create_producers(number_of_producers,queue,lock,current_bytes_try):
    """Creates a list of producers that share a queue and a lock.
    The producers create data for the consumers in the consumer producer model

    :param number_of_producers: The number of producers that will add to the queue
    :param queue: The process safe queue that will be shared among producers/consumers and main process
    :param lock: The lock over resources that will be shared among producers/consumers and main process
    :param current_bytes_try: The length of the byte sequence to be generated by the producers
    :return: A list of producer processes that can be started
    """
    # Create producers processes

    offset = 256 ** (current_bytes_try - 1)
    if offset == 1:
        offset = 0
    elements_to_be_added = 256 ** current_bytes_try - offset
    # TODO: Check that numbers divide correctly
    elements_per_producer = int(
        elements_to_be_added / numbers_of_producers)

    producers = []

    for i in range(numbers_of_producers):
        producers.append(Process(target=producer, args=(queue, lock, i, elements_per_producer, offset)))
    return producers


def create_consumers_and_pipes(number_of_consumers, archive_name, bytes_missing, queue, lock, file_name, file_hash, hash_method):
    """Creates a list of consumers that share a queue and a lock.
    Creates a list of pipes used for communication between consumers and main process

    :param number_of_consumers: The number of producers that will read to the queue
    :param archive_name: The name of the archive to be reconstructed by consumers
    :param bytes_missing: The number of bytes to be trimmed from the archive
    :param queue: The process safe queue that will be shared among producers/consumers and main process
    :param lock: The lock over resources that will be shared among producers/consumers and main process
    :param file_name: The name of the file to be extracted from the archive
    :param file_hash: The hash of the file to be extracted from the file
    :param hash_method: The hash method (any hashlib method sent as a string e.g. 'md5')
    :return: A list of consumers that can be started, A list of pipes used for communication between consumers and main process
    """
    # Create consumers processes

    consumers = []
    pipe_list = []

    for i in range(numbers_of_consumers):
        corrupted_archive = trim_archive(archive_name, bytes_missing, copy=True, c_name=f'tr_{i}')
        parent_conn, child_conn = Pipe()
        pipe_list.append(parent_conn)
        c = Process(target=consumer,
                    args=(queue, lock, child_conn, corrupted_archive, file_name, file_hash, hash_method))
        consumers.append(c)
    return consumers, pipe_list


if __name__ == '__main__':
    archive_name = "./the.zip"
    file_name = 'LoremIpsum.txt'
    bytes_missing = 2
    hash_method = 'md5'

    file_hash = compute_hash_unopened_file(file_name, hash_method)

    queue = Queue()
    lock = Lock()

    current_bytes_try = 1
    numbers_of_producers = 4
    numbers_of_consumers = 4

    while True:

        main_corrupted_archive, removed_bits = trim_archive(archive_name, bytes_missing, copy=True,
                                                            c_name=f'MAIN_truncated_archive', save_bits=True)
        print("Bits that were removed:", removed_bits)
        print("Generator value for the removed part:", int.from_bytes(removed_bits, byteorder='big'))

        producers = create_producers(numbers_of_producers,queue,lock,current_bytes_try)
        consumers,pipe_list= create_consumers_and_pipes(numbers_of_consumers, archive_name, bytes_missing, queue, lock, file_name, file_hash, hash_method)
        for p in producers:
            p.start()
        for c in consumers:
            c.start()
        for p in producers:
            p.join()
            print(f"Producer {p} finished the job")

        with lock:
            print(f'Main thread tries to gather results from processes')
        processes_responses = [x.recv() for x in pipe_list]

        for c in consumers:
            c.join()
            print(f"Consumer {c} finished the job")

        print("Results from processes:", processes_responses)
        for response in processes_responses:
            if response != 'Not found':
                append_bytes_to_file(main_corrupted_archive, response)
                print("File found after adding the following bits:")
                print(response)
                z = zipfile.ZipFile(main_corrupted_archive)
                f = z.open(file_name)
                f.seek(0)
                print("File content:")
                print(f.read())
                print("Done!")
                exit(0)
        print(f"Failed to unpack with {current_bytes_try}")
        current_bytes_try += 1
